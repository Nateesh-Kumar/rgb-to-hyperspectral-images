{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11541851,"sourceType":"datasetVersion","datasetId":7238156}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nimport cv2\n\n# Define paths to dataset\nbase_path = \"/kaggle/input/hsi-skincancer-main\"  # Adjust based on Kaggle dataset path\ntrain_path = os.path.join(base_path, \"train\")\n\n# Parameters\nIMG_SIZE = 128  # Reduced from 256 to 128 to save memory\nBATCH_SIZE = 8  # Reduced to handle 3D data and fit GPU memory\nNUM_CLASSES = 3  # Only class_3, class_4, class_5\nEPOCHS = 30  # Increased to 30 for more training\nCLASS_MAPPING = {3: 0, 4: 1, 5: 2}  # Map class_3 -> 0, class_4 -> 1, class_5 -> 2\nTEST_SIZE = 0.3  # 30% for test, 70% for train\n\n# Function to load and preprocess .npy files for 3D CNN\ndef load_and_preprocess_npy(file_path, img_size=IMG_SIZE):\n    # Load hyperspectral image (shape: 31, 256, 256)\n    img = np.load(file_path)\n    \n    # Transpose to (256, 256, 31) and resize spatial dimensions\n    img = np.transpose(img, (1, 2, 0))  # Shape: (256, 256, 31)\n    img = cv2.resize(img, (img_size, img_size))  # Shape: (128, 128, 31)\n    \n    # Transpose to (31, 128, 128) for depth as first dimension, then add channel dimension\n    img = np.transpose(img, (2, 0, 1))  # Shape: (31, 128, 128)\n    img = np.expand_dims(img, axis=-1)  # Shape: (31, 128, 128, 1)\n    \n    # Normalize to [0, 1]\n    img = img / np.max(img)\n    return img\n\n# Function to load all file paths and labels\ndef load_data_paths(data_path, model_type):\n    model_path = os.path.join(data_path, model_type)\n    classes = [f\"class_{i}\" for i in [3, 4, 5]]  # Only class_3, class_4, class_5\n    \n    file_paths = []\n    labels = []\n    \n    for class_name in classes:\n        class_idx = CLASS_MAPPING[int(class_name.split('_')[1])]\n        class_path = os.path.join(model_path, class_name)\n        if not os.path.exists(class_path):\n            print(f\"Warning: {class_path} does not exist\")\n            continue\n            \n        npy_files = [f for f in os.listdir(class_path) if f.endswith(\".npy\")]\n        for npy_file in npy_files:\n            file_path = os.path.join(class_path, npy_file)\n            file_paths.append(file_path)\n            labels.append(class_idx)\n    \n    return file_paths, labels\n\n# Data generator to load data in batches\ndef data_generator(file_paths, labels, img_size=IMG_SIZE, batch_size=BATCH_SIZE):\n    while True:\n        indices = np.arange(len(file_paths))\n        np.random.shuffle(indices)  # Shuffle for randomness\n        \n        for start_idx in range(0, len(file_paths), batch_size):\n            batch_indices = indices[start_idx:start_idx + batch_size]\n            X_batch = []\n            y_batch = []\n            \n            for idx in batch_indices:\n                img = load_and_preprocess_npy(file_paths[idx], img_size)\n                X_batch.append(img)\n                y_batch.append(labels[idx])\n            \n            if X_batch:  # Ensure batch is not empty\n                # Transpose to (batch_size, 128, 128, 31, 1) for 3D CNN\n                X_batch = np.transpose(np.array(X_batch), (0, 2, 3, 1, 4))  # Shape: (batch_size, 128, 128, 31, 1)\n                yield (X_batch,  # Shape: (batch_size, 128, 128, 31, 1)\n                       tf.keras.utils.to_categorical(y_batch, NUM_CLASSES))\n\n# Build an optimized 3D CNN model\ndef build_3d_cnn(input_shape=(IMG_SIZE, IMG_SIZE, 31, 1), num_classes=NUM_CLASSES):\n    model = models.Sequential([\n        layers.Input(shape=input_shape),  # Explicitly define input shape (height, width, depth, channels)\n        layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),  # Reduced pooling in depth to preserve bands\n        layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),\n        layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n                  loss='categorical_crossentropy', \n                  metrics=['accuracy'])\n    return model\n\n# Function to evaluate the model and compute metrics\ndef evaluate_model(model, test_file_paths, test_labels):\n    test_gen = data_generator(test_file_paths, test_labels, batch_size=1)\n    \n    y_true = []\n    y_pred = []\n    \n    for _ in range(len(test_file_paths)):\n        X, y = next(test_gen)\n        pred = model.predict(X, verbose=0)\n        y_true.append(np.argmax(y, axis=1)[0])\n        y_pred.append(np.argmax(pred, axis=1)[0])\n    \n    # Compute metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro')\n    recall = recall_score(y_true, y_pred, average='macro')\n    f1 = f1_score(y_true, y_pred, average='macro')\n    \n    # Generate classification report\n    report = classification_report(y_true, y_pred, labels=[0, 1, 2], target_names=[f'class_{i+3}' for i in range(3)], digits=4)\n    \n    return accuracy, precision, recall, f1, report\n\n# Main execution for awan dataset\nmodel_type = \"awan\"\n\nprint(f\"\\nTraining on {model_type} dataset...\")\n\n# Load all file paths and labels\nfile_paths, labels = load_data_paths(train_path, model_type)\n\nif not file_paths:\n    print(f\"No data found for {model_type}. Skipping...\")\nelse:\n    # Split into 70% train and 30% test, stratified to maintain class balance\n    train_paths, test_paths, train_labels, test_labels = train_test_split(\n        file_paths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n    )\n    \n    # Train the model\n    train_gen = data_generator(train_paths, train_labels, batch_size=BATCH_SIZE)\n    steps_per_epoch = len(train_paths) // BATCH_SIZE\n    cnn_model = build_3d_cnn()\n    cnn_model.fit(train_gen, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, verbose=1)\n    \n    # Evaluate on test set\n    print(f\"Evaluating on {model_type} test split...\")\n    accuracy, precision, recall, f1, report = evaluate_model(cnn_model, test_paths, test_labels)\n    \n    # Print epochs and evaluation metrics\n    print(f\"Epochs: {EPOCHS}\")\n    print(\"\\nEvaluation Metrics:\")\n    print(f\"Accuracy      : {accuracy:.4f}\")\n    print(f\"Precision (macro): {precision:.4f}\")\n    print(f\"Recall (macro)   : {recall:.4f}\")\n    print(f\"F1 Score (macro) : {f1:.4f}\")\n    print(\"\\nFull Classification Report:\")\n    print(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T12:34:39.116256Z","iopub.execute_input":"2025-04-29T12:34:39.116515Z","iopub.status.idle":"2025-04-29T12:50:30.301331Z","shell.execute_reply.started":"2025-04-29T12:34:39.116496Z","shell.execute_reply":"2025-04-29T12:50:30.300486Z"}},"outputs":[{"name":"stdout","text":"\nTraining on awan dataset...\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745930084.794529      89 service.cc:148] XLA service 0x7cd148007860 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745930084.795566      89 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1745930085.099109      89 cuda_dnn.cc:529] Loaded cuDNN version 90300\n2025-04-29 12:34:51.324329: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[64,32,3,3,3]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,64,64,31]{4,3,2,1,0}, f32[8,64,64,64,31]{4,3,2,1,0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n2025-04-29 12:34:53.528219: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 3.20400787s\nTrying algorithm eng0{} for conv (f32[64,32,3,3,3]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,64,64,31]{4,3,2,1,0}, f32[8,64,64,64,31]{4,3,2,1,0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\nI0000 00:00:1745930098.561319      89 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 510ms/step - accuracy: 0.4289 - loss: 1.8261\nEpoch 2/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 228ms/step - accuracy: 0.6250 - loss: 0.8537\nEpoch 3/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.6927 - loss: 0.7302\nEpoch 4/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.7212 - loss: 0.6663\nEpoch 5/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.7654 - loss: 0.5630\nEpoch 6/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.8357 - loss: 0.4369\nEpoch 7/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8484 - loss: 0.3859\nEpoch 8/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8767 - loss: 0.3514\nEpoch 9/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9197 - loss: 0.2334\nEpoch 10/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9251 - loss: 0.2282\nEpoch 11/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9703 - loss: 0.1211\nEpoch 12/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9250 - loss: 0.2137\nEpoch 13/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9559 - loss: 0.1511\nEpoch 14/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9761 - loss: 0.0798\nEpoch 15/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9694 - loss: 0.0825\nEpoch 16/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9891 - loss: 0.0432\nEpoch 17/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9925 - loss: 0.0306\nEpoch 18/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9691 - loss: 0.1192\nEpoch 19/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9666 - loss: 0.1149\nEpoch 20/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9938 - loss: 0.0402\nEpoch 21/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9907 - loss: 0.0377\nEpoch 22/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9978 - loss: 0.0162\nEpoch 23/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9991 - loss: 0.0090\nEpoch 24/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9984 - loss: 0.0058\nEpoch 25/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9960 - loss: 0.0658\nEpoch 26/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9996 - loss: 0.0114\nEpoch 27/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9976 - loss: 0.0171\nEpoch 28/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9974 - loss: 0.0242\nEpoch 29/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0079\nEpoch 30/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9961 - loss: 0.0116\nEvaluating on awan test split...\nEpochs: 30\n\nEvaluation Metrics:\nAccuracy      : 0.6370\nPrecision (macro): 0.6363\nRecall (macro)   : 0.6370\nF1 Score (macro) : 0.6357\n\nFull Classification Report:\n              precision    recall  f1-score   support\n\n     class_3     0.6718    0.6377    0.6543       138\n     class_4     0.5878    0.5540    0.5704       139\n     class_5     0.6494    0.7194    0.6826       139\n\n    accuracy                         0.6370       416\n   macro avg     0.6363    0.6370    0.6357       416\nweighted avg     0.6362    0.6370    0.6357       416\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nimport cv2\n\n# Define paths to dataset\nbase_path = \"/kaggle/input/hsi-skincancer-main\"  # Adjust based on Kaggle dataset path\ntrain_path = os.path.join(base_path, \"train\")\n\n# Parameters\nIMG_SIZE = 128  # Reduced from 256 to 128 to save memory\nBATCH_SIZE = 8  # Reduced to handle 3D data and fit GPU memory\nNUM_CLASSES = 3  # Only class_3, class_4, class_5\nEPOCHS = 50  # Increased to 30 for more training\nCLASS_MAPPING = {3: 0, 4: 1, 5: 2}  # Map class_3 -> 0, class_4 -> 1, class_5 -> 2\nTEST_SIZE = 0.3  # 30% for test, 70% for train\n\n# Function to load and preprocess .npy files for 3D CNN\ndef load_and_preprocess_npy(file_path, img_size=IMG_SIZE):\n    # Load hyperspectral image (shape: 31, 256, 256)\n    img = np.load(file_path)\n    \n    # Transpose to (256, 256, 31) and resize spatial dimensions\n    img = np.transpose(img, (1, 2, 0))  # Shape: (256, 256, 31)\n    img = cv2.resize(img, (img_size, img_size))  # Shape: (128, 128, 31)\n    \n    # Transpose to (31, 128, 128) for depth as first dimension, then add channel dimension\n    img = np.transpose(img, (2, 0, 1))  # Shape: (31, 128, 128)\n    img = np.expand_dims(img, axis=-1)  # Shape: (31, 128, 128, 1)\n    \n    # Normalize to [0, 1]\n    img = img / np.max(img)\n    return img\n\n# Function to load all file paths and labels\ndef load_data_paths(data_path, model_type):\n    model_path = os.path.join(data_path, model_type)\n    classes = [f\"class_{i}\" for i in [3, 4, 5]]  # Only class_3, class_4, class_5\n    \n    file_paths = []\n    labels = []\n    \n    for class_name in classes:\n        class_idx = CLASS_MAPPING[int(class_name.split('_')[1])]\n        class_path = os.path.join(model_path, class_name)\n        if not os.path.exists(class_path):\n            print(f\"Warning: {class_path} does not exist\")\n            continue\n            \n        npy_files = [f for f in os.listdir(class_path) if f.endswith(\".npy\")]\n        for npy_file in npy_files:\n            file_path = os.path.join(class_path, npy_file)\n            file_paths.append(file_path)\n            labels.append(class_idx)\n    \n    return file_paths, labels\n\n# Data generator to load data in batches\ndef data_generator(file_paths, labels, img_size=IMG_SIZE, batch_size=BATCH_SIZE):\n    while True:\n        indices = np.arange(len(file_paths))\n        np.random.shuffle(indices)  # Shuffle for randomness\n        \n        for start_idx in range(0, len(file_paths), batch_size):\n            batch_indices = indices[start_idx:start_idx + batch_size]\n            X_batch = []\n            y_batch = []\n            \n            for idx in batch_indices:\n                img = load_and_preprocess_npy(file_paths[idx], img_size)\n                X_batch.append(img)\n                y_batch.append(labels[idx])\n            \n            if X_batch:  # Ensure batch is not empty\n                # Transpose to (batch_size, 128, 128, 31, 1) for 3D CNN\n                X_batch = np.transpose(np.array(X_batch), (0, 2, 3, 1, 4))  # Shape: (batch_size, 128, 128, 31, 1)\n                yield (X_batch,  # Shape: (batch_size, 128, 128, 31, 1)\n                       tf.keras.utils.to_categorical(y_batch, NUM_CLASSES))\n\n# Build an optimized 3D CNN model\ndef build_3d_cnn(input_shape=(IMG_SIZE, IMG_SIZE, 31, 1), num_classes=NUM_CLASSES):\n    model = models.Sequential([\n        layers.Input(shape=input_shape),  # Explicitly define input shape (height, width, depth, channels)\n        layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),  # Reduced pooling in depth to preserve bands\n        layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),\n        layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n                  loss='categorical_crossentropy', \n                  metrics=['accuracy'])\n    return model\n\n# Function to evaluate the model and compute metrics\ndef evaluate_model(model, test_file_paths, test_labels):\n    test_gen = data_generator(test_file_paths, test_labels, batch_size=1)\n    \n    y_true = []\n    y_pred = []\n    \n    for _ in range(len(test_file_paths)):\n        X, y = next(test_gen)\n        pred = model.predict(X, verbose=0)\n        y_true.append(np.argmax(y, axis=1)[0])\n        y_pred.append(np.argmax(pred, axis=1)[0])\n    \n    # Compute metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro')\n    recall = recall_score(y_true, y_pred, average='macro')\n    f1 = f1_score(y_true, y_pred, average='macro')\n    \n    # Generate classification report\n    report = classification_report(y_true, y_pred, labels=[0, 1, 2], target_names=[f'class_{i+3}' for i in range(3)], digits=4)\n    \n    return accuracy, precision, recall, f1, report\n\n# Main execution for hrnet dataset\nmodel_type = \"hrnet\"\n\nprint(f\"\\nTraining on {model_type} dataset...\")\n\n# Load all file paths and labels\nfile_paths, labels = load_data_paths(train_path, model_type)\n\nif not file_paths:\n    print(f\"No data found for {model_type}. Skipping...\")\nelse:\n    # Split into 70% train and 30% test, stratified to maintain class balance\n    train_paths, test_paths, train_labels, test_labels = train_test_split(\n        file_paths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n    )\n    \n    # Train the model\n    train_gen = data_generator(train_paths, train_labels, batch_size=BATCH_SIZE)\n    steps_per_epoch = len(train_paths) // BATCH_SIZE\n    cnn_model = build_3d_cnn()\n    cnn_model.fit(train_gen, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, verbose=1)\n    \n    # Evaluate on test set\n    print(f\"Evaluating on {model_type} test split...\")\n    accuracy, precision, recall, f1, report = evaluate_model(cnn_model, test_paths, test_labels)\n    \n    # Print epochs and evaluation metrics\n    print(f\"Epochs: {EPOCHS}\")\n    print(\"\\nEvaluation Metrics:\")\n    print(f\"Accuracy      : {accuracy:.4f}\")\n    print(f\"Precision (macro): {precision:.4f}\")\n    print(f\"Recall (macro)   : {recall:.4f}\")\n    print(f\"F1 Score (macro) : {f1:.4f}\")\n    print(\"\\nFull Classification Report:\")\n    print(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T19:23:37.293136Z","iopub.execute_input":"2025-04-29T19:23:37.293945Z","iopub.status.idle":"2025-04-29T19:48:51.891640Z","shell.execute_reply.started":"2025-04-29T19:23:37.293908Z","shell.execute_reply":"2025-04-29T19:48:51.890753Z"}},"outputs":[{"name":"stderr","text":"2025-04-29 19:23:38.914673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745954619.151174      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745954619.210869      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\nTraining on hrnet dataset...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745954632.817575      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745954639.828626      91 service.cc:148] XLA service 0x7a2b98005eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745954639.829488      91 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1745954640.135136      91 cuda_dnn.cc:529] Loaded cuDNN version 90300\n2025-04-29 19:24:06.437148: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[64,32,3,3,3]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,64,64,31]{4,3,2,1,0}, f32[8,64,64,64,31]{4,3,2,1,0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n2025-04-29 19:24:08.640565: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 3.203610015s\nTrying algorithm eng0{} for conv (f32[64,32,3,3,3]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,64,64,31]{4,3,2,1,0}, f32[8,64,64,64,31]{4,3,2,1,0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\nI0000 00:00:1745954653.753190      91 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 471ms/step - accuracy: 0.4791 - loss: 1.0724\nEpoch 2/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 228ms/step - accuracy: 0.6943 - loss: 0.7365\nEpoch 3/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.7134 - loss: 0.6829\nEpoch 4/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8225 - loss: 0.4991\nEpoch 5/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8381 - loss: 0.4541\nEpoch 6/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.8222 - loss: 0.4709\nEpoch 7/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.8794 - loss: 0.3373\nEpoch 8/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9134 - loss: 0.2245\nEpoch 9/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9271 - loss: 0.2025\nEpoch 10/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9391 - loss: 0.1643\nEpoch 11/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9580 - loss: 0.1190\nEpoch 12/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9567 - loss: 0.1248\nEpoch 13/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9597 - loss: 0.1027\nEpoch 14/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9720 - loss: 0.0770\nEpoch 15/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9778 - loss: 0.0638\nEpoch 16/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9871 - loss: 0.0530\nEpoch 17/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9897 - loss: 0.0328\nEpoch 18/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9955 - loss: 0.0304\nEpoch 19/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9820 - loss: 0.0527\nEpoch 20/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9871 - loss: 0.0305\nEpoch 21/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9975 - loss: 0.0144\nEpoch 22/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9992 - loss: 0.0130\nEpoch 23/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9910 - loss: 0.0405\nEpoch 24/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9962 - loss: 0.0162\nEpoch 25/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9938 - loss: 0.0333\nEpoch 26/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9999 - loss: 0.0050\nEpoch 27/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9928 - loss: 0.0309\nEpoch 28/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0072\nEpoch 29/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9953 - loss: 0.0153\nEpoch 30/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9863 - loss: 0.0233\nEpoch 31/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9941 - loss: 0.0116\nEpoch 32/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0065\nEpoch 33/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9988 - loss: 0.0049\nEpoch 34/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9899 - loss: 0.0386\nEpoch 35/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9971 - loss: 0.0087\nEpoch 36/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0015\nEpoch 37/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9912 - loss: 0.0270\nEpoch 38/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9930 - loss: 0.0255\nEpoch 39/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9980 - loss: 0.0067\nEpoch 40/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0025\nEpoch 41/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0018\nEpoch 42/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9937 - loss: 0.0116\nEpoch 43/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9926 - loss: 0.0181\nEpoch 44/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9991 - loss: 0.0105\nEpoch 45/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9989 - loss: 0.0037\nEpoch 46/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0021\nEpoch 47/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9990 - loss: 0.0038\nEpoch 48/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9958 - loss: 0.0180\nEpoch 49/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9888 - loss: 0.0325\nEpoch 50/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9933 - loss: 0.0224\nEvaluating on hrnet test split...\nEpochs: 50\n\nEvaluation Metrics:\nAccuracy      : 0.7188\nPrecision (macro): 0.7187\nRecall (macro)   : 0.7187\nF1 Score (macro) : 0.7165\n\nFull Classification Report:\n              precision    recall  f1-score   support\n\n     class_3     0.7638    0.7029    0.7321       138\n     class_4     0.6719    0.6187    0.6442       139\n     class_5     0.7205    0.8345    0.7733       139\n\n    accuracy                         0.7188       416\n   macro avg     0.7187    0.7187    0.7165       416\nweighted avg     0.7186    0.7188    0.7165       416\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nimport cv2\n\n# Define paths to dataset\nbase_path = \"/kaggle/input/hsi-skincancer-main\"  # Adjust based on Kaggle dataset path\ntrain_path = os.path.join(base_path, \"train\")\n\n# Parameters\nIMG_SIZE = 128  # Reduced from 256 to 128 to save memory\nBATCH_SIZE = 8  # Reduced to handle 3D data and fit GPU memory\nNUM_CLASSES = 3  # Only class_3, class_4, class_5\nEPOCHS = 30  # Increased to 30 for more training\nCLASS_MAPPING = {3: 0, 4: 1, 5: 2}  # Map class_3 -> 0, class_4 -> 1, class_5 -> 2\nTEST_SIZE = 0.3  # 30% for test, 70% for train\n\n# Function to load and preprocess .npy files for 3D CNN\ndef load_and_preprocess_npy(file_path, img_size=IMG_SIZE):\n    # Load hyperspectral image (shape: 31, 256, 256)\n    img = np.load(file_path)\n    \n    # Transpose to (256, 256, 31) and resize spatial dimensions\n    img = np.transpose(img, (1, 2, 0))  # Shape: (256, 256, 31)\n    img = cv2.resize(img, (img_size, img_size))  # Shape: (128, 128, 31)\n    \n    # Transpose to (31, 128, 128) for depth as first dimension, then add channel dimension\n    img = np.transpose(img, (2, 0, 1))  # Shape: (31, 128, 128)\n    img = np.expand_dims(img, axis=-1)  # Shape: (31, 128, 128, 1)\n    \n    # Normalize to [0, 1]\n    img = img / np.max(img)\n    return img\n\n# Function to load all file paths and labels\ndef load_data_paths(data_path, model_type):\n    model_path = os.path.join(data_path, model_type)\n    classes = [f\"class_{i}\" for i in [3, 4, 5]]  # Only class_3, class_4, class_5\n    \n    file_paths = []\n    labels = []\n    \n    for class_name in classes:\n        class_idx = CLASS_MAPPING[int(class_name.split('_')[1])]\n        class_path = os.path.join(model_path, class_name)\n        if not os.path.exists(class_path):\n            print(f\"Warning: {class_path} does not exist\")\n            continue\n            \n        npy_files = [f for f in os.listdir(class_path) if f.endswith(\".npy\")]\n        for npy_file in npy_files:\n            file_path = os.path.join(class_path, npy_file)\n            file_paths.append(file_path)\n            labels.append(class_idx)\n    \n    return file_paths, labels\n\n# Data generator to load data in batches\ndef data_generator(file_paths, labels, img_size=IMG_SIZE, batch_size=BATCH_SIZE):\n    while True:\n        indices = np.arange(len(file_paths))\n        np.random.shuffle(indices)  # Shuffle for randomness\n        \n        for start_idx in range(0, len(file_paths), batch_size):\n            batch_indices = indices[start_idx:start_idx + batch_size]\n            X_batch = []\n            y_batch = []\n            \n            for idx in batch_indices:\n                img = load_and_preprocess_npy(file_paths[idx], img_size)\n                X_batch.append(img)\n                y_batch.append(labels[idx])\n            \n            if X_batch:  # Ensure batch is not empty\n                # Transpose to (batch_size, 128, 128, 31, 1) for 3D CNN\n                X_batch = np.transpose(np.array(X_batch), (0, 2, 3, 1, 4))  # Shape: (batch_size, 128, 128, 31, 1)\n                yield (X_batch,  # Shape: (batch_size, 128, 128, 31, 1)\n                       tf.keras.utils.to_categorical(y_batch, NUM_CLASSES))\n\n# Build an optimized 3D CNN model\ndef build_3d_cnn(input_shape=(IMG_SIZE, IMG_SIZE, 31, 1), num_classes=NUM_CLASSES):\n    model = models.Sequential([\n        layers.Input(shape=input_shape),  # Explicitly define input shape (height, width, depth, channels)\n        layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),  # Reduced pooling in depth to preserve bands\n        layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),\n        layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n                  loss='categorical_crossentropy', \n                  metrics=['accuracy'])\n    return model\n\n# Function to evaluate the model and compute metrics\ndef evaluate_model(model, test_file_paths, test_labels):\n    test_gen = data_generator(test_file_paths, test_labels, batch_size=1)\n    \n    y_true = []\n    y_pred = []\n    \n    for _ in range(len(test_file_paths)):\n        X, y = next(test_gen)\n        pred = model.predict(X, verbose=0)\n        y_true.append(np.argmax(y, axis=1)[0])\n        y_pred.append(np.argmax(pred, axis=1)[0])\n    \n    # Compute metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro')\n    recall = recall_score(y_true, y_pred, average='macro')\n    f1 = f1_score(y_true, y_pred, average='macro')\n    \n    # Generate classification report\n    report = classification_report(y_true, y_pred, labels=[0, 1, 2], target_names=[f'class_{i+3}' for i in range(3)], digits=4)\n    \n    return accuracy, precision, recall, f1, report\n\n# Main execution for hscnn_plus dataset\nmodel_type = \"hscnn_plus\"\n\nprint(f\"\\nTraining on {model_type} dataset...\")\n\n# Load all file paths and labels\nfile_paths, labels = load_data_paths(train_path, model_type)\n\nif not file_paths:\n    print(f\"No data found for {model_type}. Skipping...\")\nelse:\n    # Split into 70% train and 30% test, stratified to maintain class balance\n    train_paths, test_paths, train_labels, test_labels = train_test_split(\n        file_paths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n    )\n    \n    # Train the model\n    train_gen = data_generator(train_paths, train_labels, batch_size=BATCH_SIZE)\n    steps_per_epoch = len(train_paths) // BATCH_SIZE\n    cnn_model = build_3d_cnn()\n    cnn_model.fit(train_gen, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, verbose=1)\n    \n    # Evaluate on test set\n    print(f\"Evaluating on {model_type} test split...\")\n    accuracy, precision, recall, f1, report = evaluate_model(cnn_model, test_paths, test_labels)\n    \n    # Print epochs and evaluation metrics\n    print(f\"Epochs: {EPOCHS}\")\n    print(\"\\nEvaluation Metrics:\")\n    print(f\"Accuracy      : {accuracy:.4f}\")\n    print(f\"Precision (macro): {precision:.4f}\")\n    print(f\"Recall (macro)   : {recall:.4f}\")\n    print(f\"F1 Score (macro) : {f1:.4f}\")\n    print(\"\\nFull Classification Report:\")\n    print(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:22:27.749662Z","iopub.execute_input":"2025-04-29T13:22:27.750166Z","iopub.status.idle":"2025-04-29T13:38:42.939227Z","shell.execute_reply.started":"2025-04-29T13:22:27.750142Z","shell.execute_reply":"2025-04-29T13:38:42.938449Z"}},"outputs":[{"name":"stderr","text":"2025-04-29 13:22:29.485431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745932949.680983      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745932949.737790      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\nTraining on hscnn_plus dataset...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745932963.128023      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745932970.190550      91 service.cc:148] XLA service 0x7b3e3c0053f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745932970.191456      91 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1745932970.486825      91 cuda_dnn.cc:529] Loaded cuDNN version 90300\n2025-04-29 13:22:56.712925: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[64,32,3,3,3]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,64,64,31]{4,3,2,1,0}, f32[8,64,64,64,31]{4,3,2,1,0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n2025-04-29 13:22:58.922831: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 3.210064795s\nTrying algorithm eng0{} for conv (f32[64,32,3,3,3]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,64,64,31]{4,3,2,1,0}, f32[8,64,64,64,31]{4,3,2,1,0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\nI0000 00:00:1745932983.997674      91 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 541ms/step - accuracy: 0.5211 - loss: 1.1338\nEpoch 2/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 228ms/step - accuracy: 0.7045 - loss: 0.6957\nEpoch 3/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.7582 - loss: 0.5815\nEpoch 4/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8264 - loss: 0.4590\nEpoch 5/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8590 - loss: 0.3650\nEpoch 6/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9212 - loss: 0.2289\nEpoch 7/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9526 - loss: 0.1808\nEpoch 8/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9527 - loss: 0.1259\nEpoch 9/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9752 - loss: 0.0629\nEpoch 10/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9735 - loss: 0.0951\nEpoch 11/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9770 - loss: 0.0623\nEpoch 12/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9985 - loss: 0.0146\nEpoch 13/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9944 - loss: 0.0186\nEpoch 14/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9925 - loss: 0.0199\nEpoch 15/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9884 - loss: 0.0435\nEpoch 16/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9894 - loss: 0.0484\nEpoch 17/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9980 - loss: 0.0099\nEpoch 18/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9927 - loss: 0.0238\nEpoch 19/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9947 - loss: 0.0136\nEpoch 20/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9903 - loss: 0.0296\nEpoch 21/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9939 - loss: 0.0182\nEpoch 22/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9976 - loss: 0.0174\nEpoch 23/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9973 - loss: 0.0123\nEpoch 24/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9957 - loss: 0.0194\nEpoch 25/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0083\nEpoch 26/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9877 - loss: 0.0242\nEpoch 27/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9963 - loss: 0.0165\nEpoch 28/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9910 - loss: 0.0317\nEpoch 29/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9984 - loss: 0.0078\nEpoch 30/30\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0036\nEvaluating on hscnn_plus test split...\nEpochs: 30\n\nEvaluation Metrics:\nAccuracy      : 0.6875\nPrecision (macro): 0.6897\nRecall (macro)   : 0.6874\nF1 Score (macro) : 0.6867\n\nFull Classification Report:\n              precision    recall  f1-score   support\n\n     class_3     0.7417    0.6449    0.6899       138\n     class_4     0.6241    0.6331    0.6286       139\n     class_5     0.7032    0.7842    0.7415       139\n\n    accuracy                         0.6875       416\n   macro avg     0.6897    0.6874    0.6867       416\nweighted avg     0.6895    0.6875    0.6867       416\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nimport cv2\n\n# Define paths to dataset\nbase_path = \"/kaggle/input/hsi-skincancer-main\"  # Adjust based on Kaggle dataset path\ntrain_path = os.path.join(base_path, \"train\")\n\n# Parameters\nIMG_SIZE = 128  # Reduced from 256 to 128 to save memory\nBATCH_SIZE = 8  # Reduced to handle 3D data and fit GPU memory\nNUM_CLASSES = 3  # Only class_3, class_4, class_5\nEPOCHS = 50  # Increased to 30 for more training\nCLASS_MAPPING = {3: 0, 4: 1, 5: 2}  # Map class_3 -> 0, class_4 -> 1, class_5 -> 2\nTEST_SIZE = 0.3  # 30% for test, 70% for train\n\n# Function to load and preprocess .npy files for 3D CNN\ndef load_and_preprocess_npy(file_path, img_size=IMG_SIZE):\n    # Load hyperspectral image (shape: 31, 256, 256)\n    img = np.load(file_path)\n    \n    # Transpose to (256, 256, 31) and resize spatial dimensions\n    img = np.transpose(img, (1, 2, 0))  # Shape: (256, 256, 31)\n    img = cv2.resize(img, (img_size, img_size))  # Shape: (128, 128, 31)\n    \n    # Transpose to (31, 128, 128) for depth as first dimension, then add channel dimension\n    img = np.transpose(img, (2, 0, 1))  # Shape: (31, 128, 128)\n    img = np.expand_dims(img, axis=-1)  # Shape: (31, 128, 128, 1)\n    \n    # Normalize to [0, 1]\n    img = img / np.max(img)\n    return img\n\n# Function to load all file paths and labels\ndef load_data_paths(data_path, model_type):\n    model_path = os.path.join(data_path, model_type)\n    classes = [f\"class_{i}\" for i in [3, 4, 5]]  # Only class_3, class_4, class_5\n    \n    file_paths = []\n    labels = []\n    \n    for class_name in classes:\n        class_idx = CLASS_MAPPING[int(class_name.split('_')[1])]\n        class_path = os.path.join(model_path, class_name)\n        if not os.path.exists(class_path):\n            print(f\"Warning: {class_path} does not exist\")\n            continue\n            \n        npy_files = [f for f in os.listdir(class_path) if f.endswith(\".npy\")]\n        for npy_file in npy_files:\n            file_path = os.path.join(class_path, npy_file)\n            file_paths.append(file_path)\n            labels.append(class_idx)\n    \n    return file_paths, labels\n\n# Data generator to load data in batches\ndef data_generator(file_paths, labels, img_size=IMG_SIZE, batch_size=BATCH_SIZE):\n    while True:\n        indices = np.arange(len(file_paths))\n        np.random.shuffle(indices)  # Shuffle for randomness\n        \n        for start_idx in range(0, len(file_paths), batch_size):\n            batch_indices = indices[start_idx:start_idx + batch_size]\n            X_batch = []\n            y_batch = []\n            \n            for idx in batch_indices:\n                img = load_and_preprocess_npy(file_paths[idx], img_size)\n                X_batch.append(img)\n                y_batch.append(labels[idx])\n            \n            if X_batch:  # Ensure batch is not empty\n                # Transpose to (batch_size, 128, 128, 31, 1) for 3D CNN\n                X_batch = np.transpose(np.array(X_batch), (0, 2, 3, 1, 4))  # Shape: (batch_size, 128, 128, 31, 1)\n                yield (X_batch,  # Shape: (batch_size, 128, 128, 31, 1)\n                       tf.keras.utils.to_categorical(y_batch, NUM_CLASSES))\n\n# Build an optimized 3D CNN model\ndef build_3d_cnn(input_shape=(IMG_SIZE, IMG_SIZE, 31, 1), num_classes=NUM_CLASSES):\n    model = models.Sequential([\n        layers.Input(shape=input_shape),  # Explicitly define input shape (height, width, depth, channels)\n        layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),  # Reduced pooling in depth to preserve bands\n        layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),\n        layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n        layers.MaxPooling3D((2, 2, 1)),\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n                  loss='categorical_crossentropy', \n                  metrics=['accuracy'])\n    return model\n\n# Function to evaluate the model and compute metrics\ndef evaluate_model(model, test_file_paths, test_labels):\n    test_gen = data_generator(test_file_paths, test_labels, batch_size=1)\n    \n    y_true = []\n    y_pred = []\n    \n    for _ in range(len(test_file_paths)):\n        X, y = next(test_gen)\n        pred = model.predict(X, verbose=0)\n        y_true.append(np.argmax(y, axis=1)[0])\n        y_pred.append(np.argmax(pred, axis=1)[0])\n    \n    # Compute metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro')\n    recall = recall_score(y_true, y_pred, average='macro')\n    f1 = f1_score(y_true, y_pred, average='macro')\n    \n    # Generate classification report\n    report = classification_report(y_true, y_pred, labels=[0, 1, 2], target_names=[f'class_{i+3}' for i in range(3)], digits=4)\n    \n    return accuracy, precision, recall, f1, report\n\n# Main execution for mst_plus_plus dataset\nmodel_type = \"mst_plus_plus\"\n\nprint(f\"\\nTraining on {model_type} dataset...\")\n\n# Load all file paths and labels\nfile_paths, labels = load_data_paths(train_path, model_type)\n\nif not file_paths:\n    print(f\"No data found for {model_type}. Skipping...\")\nelse:\n    # Split into 70% train and 30% test, stratified to maintain class balance\n    train_paths, test_paths, train_labels, test_labels = train_test_split(\n        file_paths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n    )\n    \n    # Train the model\n    train_gen = data_generator(train_paths, train_labels, batch_size=BATCH_SIZE)\n    steps_per_epoch = len(train_paths) // BATCH_SIZE\n    cnn_model = build_3d_cnn()\n    cnn_model.fit(train_gen, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, verbose=1)\n    \n    # Evaluate on test set\n    print(f\"Evaluating on {model_type} test split...\")\n    accuracy, precision, recall, f1, report = evaluate_model(cnn_model, test_paths, test_labels)\n    \n    # Print epochs and evaluation metrics\n    print(f\"Epochs: {EPOCHS}\")\n    print(\"\\nEvaluation Metrics:\")\n    print(f\"Accuracy      : {accuracy:.4f}\")\n    print(f\"Precision (macro): {precision:.4f}\")\n    print(f\"Recall (macro)   : {recall:.4f}\")\n    print(f\"F1 Score (macro) : {f1:.4f}\")\n    print(\"\\nFull Classification Report:\")\n    print(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:55:37.628349Z","iopub.execute_input":"2025-04-29T18:55:37.629019Z","iopub.status.idle":"2025-04-29T19:21:03.139323Z","shell.execute_reply.started":"2025-04-29T18:55:37.628995Z","shell.execute_reply":"2025-04-29T19:21:03.138683Z"}},"outputs":[{"name":"stderr","text":"2025-04-29 18:55:39.189423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745952939.381478      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745952939.442929      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\nTraining on mst_plus_plus dataset...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745952952.382858      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745952958.712882      90 service.cc:148] XLA service 0x7c1eb4006190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745952958.713386      90 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1745952959.006537      90 cuda_dnn.cc:529] Loaded cuDNN version 90300\n2025-04-29 18:56:05.204770: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[64,32,3,3,3]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,64,64,31]{4,3,2,1,0}, f32[8,64,64,64,31]{4,3,2,1,0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n2025-04-29 18:56:07.409049: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 3.20437055s\nTrying algorithm eng0{} for conv (f32[64,32,3,3,3]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,64,64,31]{4,3,2,1,0}, f32[8,64,64,64,31]{4,3,2,1,0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\nI0000 00:00:1745952972.797758      90 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 537ms/step - accuracy: 0.4958 - loss: 1.1329\nEpoch 2/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 228ms/step - accuracy: 0.7248 - loss: 0.6812\nEpoch 3/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.7872 - loss: 0.5771\nEpoch 4/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8201 - loss: 0.4783\nEpoch 5/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8449 - loss: 0.4267\nEpoch 6/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8825 - loss: 0.3117\nEpoch 7/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.8870 - loss: 0.2658\nEpoch 8/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9109 - loss: 0.2514\nEpoch 9/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9604 - loss: 0.1214\nEpoch 10/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9497 - loss: 0.1229\nEpoch 11/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9878 - loss: 0.0628\nEpoch 12/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9787 - loss: 0.0774\nEpoch 13/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9899 - loss: 0.0537\nEpoch 14/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9923 - loss: 0.0343\nEpoch 15/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9775 - loss: 0.0631\nEpoch 16/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9962 - loss: 0.0177\nEpoch 17/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9972 - loss: 0.0134\nEpoch 18/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9915 - loss: 0.0296\nEpoch 19/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9955 - loss: 0.0257\nEpoch 20/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0076\nEpoch 21/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9950 - loss: 0.0093\nEpoch 22/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9992 - loss: 0.0045\nEpoch 23/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0033\nEpoch 24/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9987 - loss: 0.0060\nEpoch 25/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9944 - loss: 0.0201\nEpoch 26/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9999 - loss: 0.0061\nEpoch 27/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9926 - loss: 0.0205\nEpoch 28/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9709 - loss: 0.1094\nEpoch 29/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9993 - loss: 0.0083\nEpoch 30/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9913 - loss: 0.0302\nEpoch 31/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9931 - loss: 0.0408\nEpoch 32/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9876 - loss: 0.0599\nEpoch 33/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9999 - loss: 0.0074\nEpoch 34/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9988 - loss: 0.0072\nEpoch 35/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9997 - loss: 0.0097\nEpoch 36/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0018\nEpoch 37/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9995 - loss: 0.0024\nEpoch 38/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0029\nEpoch 39/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 6.8701e-04\nEpoch 40/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 5.3749e-04\nEpoch 41/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 5.5219e-04\nEpoch 42/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.9994 - loss: 0.0013\nEpoch 43/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 8.2239e-04\nEpoch 44/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 5.5267e-04\nEpoch 45/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 2.7523e-04\nEpoch 46/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 2.7936e-04\nEpoch 47/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 2.0020e-04\nEpoch 48/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 2.6661e-04\nEpoch 49/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 1.3191e-04\nEpoch 50/50\n\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 6.6052e-04\nEvaluating on mst_plus_plus test split...\nEpochs: 50\n\nEvaluation Metrics:\nAccuracy      : 0.7548\nPrecision (macro): 0.7534\nRecall (macro)   : 0.7547\nF1 Score (macro) : 0.7533\n\nFull Classification Report:\n              precision    recall  f1-score   support\n\n     class_3     0.7481    0.7101    0.7286       138\n     class_4     0.7239    0.6978    0.7106       139\n     class_5     0.7881    0.8561    0.8207       139\n\n    accuracy                         0.7548       416\n   macro avg     0.7534    0.7547    0.7533       416\nweighted avg     0.7534    0.7548    0.7534       416\n\n","output_type":"stream"}],"execution_count":1}]}